{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Continuous Integration for Data Scientsts</center></h1>\n",
    "<h2><center>by Jaidev Deshpande</center></h2>\n",
    "<h3><center>Data Scientist @ Cube26 Software Pvt Ltd</center></h3>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<div id=\"social\">\n",
    "<div id=\"social_twitter\">\n",
    "    <a href=\"http://twitter.com/jaidevd\"><img src=\"images/twitter-128.png\" width=\"32\" height=\"32\">\n",
    "    </a>\n",
    "</div>\n",
    "<div id=\"social_medium\">\n",
    "    <a href=\"http://medium.com/@jaidevd\"><img src=\"images/medium.ico\" width=\"32\" height=\"32\"></a>\n",
    "</div>\n",
    "<div id=\"social_github\">\n",
    "    <a href=\"http://github.com/jaidevd\"><img src=\"images/mark-github-128.png\" width=\"32\" height=\"32\"></a>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"4\"><strong>@jaidevd</strong></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Continuous Integration\" here is an all-encompassing term. By that I don't only mean CI systems, I mean the CI culture, devops, build, test and QA systems. Oh yeah, this talk is also about culture, habits and practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Peter Norvig's Q&A session on Quora:\n",
    "> I think that it will be important for machine learning experts and software engineers to come together to develop best practices for software development of machine learning systems. Currently we have a software testing regime where you define unit tests... We will need new testing processes that involve running experiments, analyzing the results... This is a great area for software engineers and machine learning people to work together to build something new and better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About me:\n",
    "\n",
    "* Data Scientist @Cube26 Software Pvt Ltd\n",
    "* Regular contributor to SciPy Stack\n",
    "* Helped develop the Canopy Data Analysis Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical Data Processing Pipeline\n",
    "![](images/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So this here is a very broadly representative, a very general data processing pipeline. That box you see is known as ETL (extract-transform and load) part of the pipeline. You start with a source datastore, and end up in a sink datastore. The ETL part of the process is the most data-intensive one - by that I don't mean that others are not, it's just that this is the part where you treat your data for what it is - raw data. After it's left this box, it's no longer raw data, it's features, it's model coefficients, it's statistics, and so on. And it is in this box that your developer side has to be the most creative, as against your data scientist side, which has to show off outside the box. So after you've extracted, transformed and loaded your data, you train some model on it, and you do some validation which might allow you to do better model selection. These steps - training, validation and model selection - are not necessarily well separated in time. You could be cross-validating one model while you are already training another model, and a third one could actually be in production that is writing the output to whatever your sink is. So the idea is to stop thinking of this as a pipeline. How do you bend the spoon? Think that there is no spoon. So there is no such pipeline, and each of these blocks are independent systems - which only happen to be loosely coupled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Data Abstractions\n",
    "## And Data Ingest as an abstraction\n",
    "![](images/wc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is a word cloud of the text of all the proposals made to PyCon. As a species, I think we are spending an enormous amount of time and money beating data into shape. That's what almost all our conferences are about. Although I cannot justify this, I have a feeling that that is really overkill. I feel like dealing with data should be easier.\n",
    "\n",
    "I've met people who very sincerely believe that if they can get a lot of data accumulated in one place, it will automatically start becoming sentient. And this is for no fault of their own. This is clearly ridiculous. Data may not be sentient, but there certainly are some advantages to thinking about it as a living entity - in that it needs to be grown and cultivated before it is harvested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Data Abstractions\n",
    "<div id=\"data_min_farm\">\n",
    "<div id=\"data_min_farm_min\">\n",
    "    <img src=\"images/mining.jpg\">\n",
    "</div>\n",
    "<h3><center>vs</center></h3>\n",
    "<div id=\"data_min_farm_farm\">\n",
    "    <img src=\"images/farming.jpg\">\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So instead of data mining, let's start speaking in terms of data farming. And here are a few ways of doing that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Data Abstractions\n",
    "\n",
    "![](images/badcode1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Data Abstractions\n",
    "## And Data Ingest as an abstraction\n",
    "\n",
    "<ul>\n",
    "    <li><h3>Raw data is an integral part of ETL and therefore of your software</h3></li>\n",
    "    <li><h3>Working off local flatflies is <em>bad</em>!</h3></li>\n",
    "    <li><h3>Learn to work from remote storage to remote storage. Use the \"cloud\".</h3></li>\n",
    "    <li><h3>What about experimental / exploratory work? Use things like sqlite!</h3></li>\n",
    "    <li><h3>Only use local files when:</h3></li>\n",
    "    <ul>\n",
    "        <li><h3>doing EDA or any interactive studies.</h3></li>\n",
    "        <li><h3>debugging a larger application.</h3></li>\n",
    "        <li><h3>prototyping (resist the temptation to deploy).</h3></li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Abstract away almost everything that has to do with ETL. We haven't done that in a long time. Machine learning systems have grown super sophisticated while data management systems (that these same machine learning systems use), have trudged behind. Let's stop thinking of raw data as an inanimate entity that you only have to dig through before you can get the gold. The digging itself is part of your software, and is subject to pretty much the same dangers as any other kind of software development. So build abstraction layers, services and all kinds of tooling that you would need around ETL. Now this might sound very obvious, but you'd be surprised at how infrequently we practice this. For example, almost all development data scientists do is based off local flatfiles. The excuse is that they're just building prototypes and it's not really their job to have to deploy their models on larger systems. But we know how fast the boundaries between data scientists and other kinds of developers are thinning. In that light, we have to learn to work with larger more intergated data sources. Even if you're just building a prototype, this is still not every healthy because you have no idea how long the prototyping is going to take, or how many intermediate files you might end up producing. So, at least as a favour to people who are deploying your work to larger systems, learn to use larger integrated systems. Even if you're using the Iris dataset, try to use sqlite instead of CSV. The cloud is your friend. The sooner you become comfortable with remote or cloud based distributed storages, the faster you can deploy your apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Ingest Libraries\n",
    "\n",
    "<ul>\n",
    "    <li><h3>sputnik: managing data packages and ML models</h3></li>\n",
    "    <li><h3>datreant - Using the local filesystem</h3></li>\n",
    "    <ul>\n",
    "        <li><h3>heterogenous data</h3></li>\n",
    "        <li><h3>many parameters, many files - lot of mess</h3></li>\n",
    "        <li><h3>datreant exploits this mess as flexibility, discoverability</h3></li>\n",
    "    </ul>\n",
    "    <li><h3>conduit: data exchange for HPC</h3></li>\n",
    "    <ul>\n",
    "        <li><h3>in-core exchange vs. file-based exchange of data</h3></li>\n",
    "        <li><h3>JSON schema to describe and annotate data</h3></li>\n",
    "    </ul>\n",
    "    <li><h3>pysemantic: automate the validation and cleaning of dataset</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here are a few things that might help you take some steps in this direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Note about the AutoML project\n",
    "![](images/automl.png)\n",
    "<h4><i>\"If machine learning is so ubiquitous, one should just be able to use it without understanding libraries.\"</i></h4>\n",
    "<h4>- Andreas Mueller @ SciPy US 2016</h4>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Note about the AutoML project\n",
    "<ul>\n",
    "    <li><h3>sklearn philosophy: explicit is better than implicit. CI philosophy - is it really? Just build and run stuff!</h3></li>\n",
    "    <li><h3>Formalize the search space mindfully - discrete or continuous? Conditional parameters?</h3></li>\n",
    "    <li><h3>Automate:</h3></li>\n",
    "    <ul>\n",
    "        <li><h3>Choice of classification/regression algorithm</h3></li>\n",
    "        <li><h3>feature extraction & preprocessing</h3></li>\n",
    "        <li><h3>hyperparameter tuning</h3></li>\n",
    "    </ul>\n",
    "    <li><h3>Randomized & Grid Search are both embarassingly parallel</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automating Model Selection\n",
    "## Automating Cross Validation and Hyperparameter Tuning\n",
    "\n",
    "```python\n",
    "class CrossValidationTask(luigi.Task):\n",
    "    \n",
    "    estimator = luigi.Parameter() # or luigi.Target\n",
    "    \n",
    "    def run(self):\n",
    "        # Run CV loop\n",
    "        # Export metrics for each iteration\n",
    "\n",
    "\n",
    "class GridSearchTask(luigi.Task):\n",
    "    \n",
    "    grid = luigi.Parameter() # or Target\n",
    "    estimator = luigi.Parameter() # or Target\n",
    "    ...\n",
    "    \n",
    "    def run(self):\n",
    "        X, y = self.input()\n",
    "        clf = GridSearchCV(self.estimator, param_grid=self.grid, ...)\n",
    "        clf.fit(X, y)\n",
    "        ...\n",
    "        joblib.dump(clf.best_estimator_, self.output())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data and Model _Quality_\n",
    "\n",
    "## (Tools from psychometrics for) data quality evaluation\n",
    "### - Katie Malone @ SciPy US 2016\n",
    "\n",
    "<ul>\n",
    "    <li><h3>Predictive modeling != building a model</h3></li>\n",
    "    <li><h3>Iterative model selection involves going all the way back to data quality, not simply changing the pipeline</h3></li>\n",
    "    <li><h3>Develop the same intuition for your data as that for your model</h3></li>\n",
    "    <ul>\n",
    "        <li><h3>complex model + mediocre dataset = fair predictive accuracy</h3></li>\n",
    "        <li><h3>simple model + great dataset = high predictive accuracy</h3></li>\n",
    "    </ul>\n",
    "    <li><h3>Build many models for the same dataset and aggregate the metrics</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data and model _Quality_\n",
    "## Communicating model results\n",
    "### - Bargava Subramanian @ SciPy US 2016\n",
    "\n",
    "<ul>\n",
    "    <li><h3>Performance metrics aren't as important as insights</h3></li>\n",
    "    <li><h3>Modularize and abstract away:</h3></li>\n",
    "    <ol>\n",
    "        <li><h3>raw data behaviour</h3></li>\n",
    "        <li><h3>visualization</h3></li>\n",
    "        <li><h3>learning and metrics</h3></li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizing (Data & ML performance)\n",
    "\n",
    "<ul>\n",
    "        <li><h3>Bokeh server for dashboards</h3></li>\n",
    "        <li><h3>Chaco / Traits-based visualizations - for interative exploration</h3></li>\n",
    "        <li><h3>Use libs like Seaborn for stats - resist the temptation to write them yourself</h3></li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exposing Trained Models\n",
    "\n",
    "<ul>\n",
    "        <li><h3>Simple serialization methods</h3></li>\n",
    "        <li><h3>sklearn-compiledtrees</h3></li>\n",
    "        <li><h3>Don't use nonlinear models where linear models will do</h3></li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exposing Trained Models - The Serverless Paradigm\n",
    "\n",
    "> Most MLaaS products don’t allow much personalisation.\n",
    "This is why the Data Science community is still “stuck” to designing ad-hoc models with lower-level tools and frameworks. Which is great. Except that most of the time these ad-hoc models are developed by data scientists and tend to get stuck at the prototyping level. The outcome of this process will require a huge effort by some other developer or engineer, who probably can’t speak the data science language.\n",
    "\n",
    "> What if prototypes developed by data scientists were production-ready without any further effort?\n",
    "\n",
    "\\- Alex Casalboni\n",
    "(PyCon Italia 2015)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exposing Trained Models - The Serverless Paradigm\n",
    "<ul>\n",
    "        <li><h3>Simple serialization methods - are not infrastructure aware</h3></li>\n",
    "        <li><h3>HTTP API? Authentication?</h3></li>\n",
    "        <li><h3>Deployment Strategies:</h3></li>\n",
    "        <ol>\n",
    "        <li><h4>Simple flask wrappers - what about model updates?</h4>\n",
    "        <li><h4>Backend scaling? No elasticity!</h4>\n",
    "        <li><h4>AWS Lambda / Amazon API Gateway - Just use a set of functions that are automatically deployed and scaled</h4>\n",
    "        </ol>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "From https://blog.alexcasalboni.com/serverless-computing-machine-learning-baf52b89e1b0#.y36241bga\n",
    "\n",
    "More slides: http://www.slideshare.net/AlexCasalboni/how-to-deploy-machine-learning-models-in-the-cloud\n",
    "\n",
    "video: https://www.youtube.com/watch?v=YsyL-3CJ9GU"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
