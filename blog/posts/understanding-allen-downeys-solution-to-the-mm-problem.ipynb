{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    ".. title: Understanding Allen Downey's Solution to the M&M Problem\n",
    ".. slug: understanding-allen-downeys-solution-to-the-mm-problem\n",
    ".. date: 2016-06-29 20:13:56 UTC+05:30\n",
    ".. tags: learning, bayes, programming, math, mathjax\n",
    ".. category: \n",
    ".. link: \n",
    ".. description: \n",
    ".. type: text\n",
    "-->\n",
    "\n",
    "Allen Downey makes a very good case for learning advanced mathematics through\n",
    "programming (Check the first section of the preface of _Think Bayes_, titled \"My theory, which is mine\").\n",
    "But before the reader can hit paydirt with using the Bayes theorem in programming,\n",
    "Downey makes you go through some elementary problems in probability, which have\n",
    "to be solved by hand first, if you expect to have a clear enough understanding\n",
    "of the concept. I can vouch for this way of learning complex concepts. The way\n",
    "I learnt the backpropagation algorithm (and its derivation), was with a pen,\n",
    "paper and a calculator.\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Downey is also very careful about pointing out the difference between how\n",
    "functions and operations manifest themselves in math and in programming. For\n",
    "example, a function (say, $f(x)$) in mathematics can be implemented in software\n",
    "by a number of things:\n",
    "\n",
    "* An array, containing only data\n",
    "* A routine that takes input(s) ($x$) and provides an output(s) ($f(x)$)\n",
    "* A symbolic expression (commonly found in [CAS](https://en.wikipedia.org/wiki/Computer_algebra_system) libraries)\n",
    "\n",
    "Not knowing these differences can severly handicap a programmer. I, for one,\n",
    "found myself stymied multiple times in the very first chapter of _Think Bayes_,\n",
    "even though I'm quite comfortable with what the Bayes theorem represents and\n",
    "what it means for problems where belief or confidence needs to keep changing\n",
    "with data. But here's the rub: I'm used to thinking about it very formally, in terms of\n",
    "continuous functions, not discrete structures. And that has been the downfall\n",
    "of many a programmer.\n",
    "\n",
    "What follows is just narcissistic note-taking, which I hope won't let me forget\n",
    "what I've already learnt.\n",
    "\n",
    "\n",
    "Bayes' Theorem\n",
    "--------------\n",
    "\n",
    "Simply put, it says that given data $D$ and a hypothesis $H$\n",
    "\n",
    "$$ \\begin{equation} P(H|D) = \\frac{P(H)P(D|H)}{P(D)} \\end{equation} $$\n",
    "\n",
    "where\n",
    "$P(H)$ is the probability of the hypothesis, or the _prior_. $P(D|H)$ is the\n",
    "_likelihood_, the probability that a favourable outcome (or anything that is\n",
    "being observed) is true under the hypothesis $H$. $P(D)$ is the normalizing\n",
    "constant, or the probability of the data being true in any case at all. The\n",
    "significance of the normalizing constant is not immediately apparent, but we\n",
    "can think of it as follows:\n",
    "\n",
    "> If there are multiple possibilities, or multiple hypothesis, then $P(D)$ is\n",
    "> the probability of an observation being true under any of them.\n",
    "\n",
    "Concretely, this means that if there are $n$ hypotheses $H_{1}$ through\n",
    "$H_{n}$, then\n",
    "\n",
    "$$ P(D) = \\sum_{i=1}^{n} P(H_{i}) P(D|H_{i}) $$\n",
    "\n",
    "\n",
    "The M&M Problem\n",
    "---------------\n",
    "\n",
    "The problem states that the distribution of colors among M&Ms before and after\n",
    "1995 is as follows:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>Color</th>\n",
    "      <th>Before 1995</th>\n",
    "      <th>After 1995</th>\n",
    "     </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>brown</td>\n",
    "      <td>30 %</td>\n",
    "      <td>13 %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>yellow</td>\n",
    "      <td>20 %</td>\n",
    "      <td>14 %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>red</td>\n",
    "      <td>20 %</td>\n",
    "      <td>13 %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>green</td>\n",
    "      <td>10 %</td>\n",
    "      <td>20 %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>orange</td>\n",
    "      <td>10 %</td>\n",
    "      <td>16 %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>tan</td>\n",
    "      <td>10 %</td>\n",
    "      <td>0 %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>blue</td>\n",
    "      <td>0 %</td>\n",
    "      <td>24 %</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "We have two bags full of M&Ms, one from 1994 and the other from 1996. We draw\n",
    "an M&M each from the two bags, without knowing which M&M came from which bag.\n",
    "One is yellow and one is green. The problem is:\n",
    "\n",
    "> What is the probability that the yellow M&M came from the 1994 bag?\n",
    "\n",
    "There can only be two hypotheses here:\n",
    "\n",
    "* Hypothesis A ($H_{a}$): The yellow M&M came from the 1994 bag, and the green one came from the 1996 bag.\n",
    "* Hypothesis B ($H_{b}$): The yellow M&M came from the 1996 bag, and the green one came from the 1994 bag.\n",
    "\n",
    "Downey also introduces a useful notation for solving such problems, where we arrange the hypotheses and their corresponding Bayesian parameters in a table as follows:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Prior</th>\n",
    "      <th>Likelihood</th>\n",
    "      <th>Normalizing Constant</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>A</th>\n",
    "      <td>?</td>\n",
    "      <td>?</td>\n",
    "      <td>?</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>B</th>\n",
    "      <td>?</td>\n",
    "      <td>?</td>\n",
    "      <td>?</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "As we proceed with this problem, we'll also see how it serves as a good example of the diachronic interpretation of the Bayes' theorem - i.e., how it helps us update our belief about a given hypothesis once we've seen more data.\n",
    "\n",
    "1. Our _priors_ about the two hypotheses would be naive, in that we would expect both hypotheses to be equiprobable, since we haven't considered the color distribution yet. Thus, $ P(H_{a}) = P(H_{b}) = \\frac{1}{2}$\n",
    "2. The likelihoods for the respective hypotheses can be readily obtained from the color distribution. Recall that $P(D|H_{a})$ is simply the probability that the observed data (one green, one yellow) is true for hypothesis $H_{a}$. Thus, $P(D|H_{a})$ equals the probability that the yellow M&M is from 1994 *and* the green one is from 1996. From the table, these values are both $\\frac{1}{5}$. Thus, $P(D|H_{a}) = \\frac{1}{25}$. Similarly, $P(D|H_{b}) = \\frac{14}{100} \\times \\frac{10}{100} = \\frac{7}{500}$.\n",
    "3. Note that for each hypothesis, the product of the first two columns makes up the numerator of the right hand side of Bayes' equation.\n",
    "\n",
    "So far so good, but I was stuck for a while before I could understand Downey's explanation of how he calculated the normalizing constant for the two cases. He writes that the third column is just the sum of the products of the first two columns. This means, that the normalizing constant is just the sum of the numerators for the respective Bayes' equations for the two scenarios. So,\n",
    "\n",
    "$$ P(D) = P(H_{a})P(D|H_{a}) + P(H_{b})P(D|H_{b})$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ P(D) = \\frac{1}{2} \\times \\frac{1}{25} + \\frac{1}{2} \\times \\frac{7}{500} = \\frac{27}{1000} $$\n",
    "\n",
    "The catch is that this equation is perfectly in line with the second equation, subject to the assumption that both hypothesis are mutually exclusive (only one can be true) and collectively exhaustive (at least one must be true).\n",
    "\n",
    "Since we now know all the values required to calculate the posteriors of both hypothesis, the rest is just arithmetic. It turns out that $P(H_{a}|D) = \\frac{20}{27}$ and $P(H_{b}|D) = \\frac{7}{27}$.\n",
    "\n",
    "This was so much fun on paper, I can't wait to use these methods in Python. Needless to say I'm pretty excited about working through _Think Bayes_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }

 },
 "nbformat": 4,
 "nbformat_minor": 0
}
